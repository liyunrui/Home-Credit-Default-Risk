{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import multiprocessing\n",
    "import warnings\n",
    "import sys\n",
    "sys.path.append('../py_model/')\n",
    "from utils import init_logging\n",
    "import logging \n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "np.random.seed(int(time.time()))\n",
    "\n",
    "CPU_USE_RATE = 0.5\n",
    "NUM_FOLDS = 5\n",
    "STRATIFIED = True  \n",
    "TEST_NULL_HYPO = False\n",
    "ITERATION = (80 if TEST_NULL_HYPO else 10) # It means how many iterations need to get the final stable AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "\n",
    "def kfold_lightgbm(df, num_folds, stratified = False):\n",
    "    '''\n",
    "    num_folds: int, how many foles u'r going to split.\n",
    "\n",
    "    Maybe we can write a helper function, to find a best parametres each time when u add a new features, to make sure reliability of experiment.\n",
    "    But, the experiement time will go up more.\n",
    "    '''\n",
    "    #---------------------\n",
    "    # Divide in training/validation and test data\n",
    "    #---------------------\n",
    "    train_df = df[df['TARGET'].notnull()]\n",
    "    test_df = df[df['TARGET'].isnull()]\n",
    "    # test_id 如果是隨機的, 會有引響麼, 理論上只要sort就好, 就算有\n",
    "    logging.info('no bugging in split' if train_df.shape[0] + test_df.shape[0] == df.shape[0] else \" opps\")\n",
    "    #---------------------\n",
    "    # core\n",
    "    #---------------------\n",
    "    logging.info(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
    "    del df\n",
    "    gc.collect()\n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=int(time.time()))\n",
    "    else:\n",
    "        folds = KFold(n_splits= num_folds, shuffle=True, random_state=int(time.time()))\n",
    "    # Create arrays and dataframes to store results\n",
    "    # train\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    train_preds = np.zeros(train_df.shape[0])\n",
    "    # test\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    # feature importance\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "\n",
    "    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "    \n",
    "    if TEST_NULL_HYPO:\n",
    "        train_df['TARGET'] = train_df['TARGET'].copy().sample(frac = 1.0).values\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n",
    "\n",
    "        # LightGBM parameters found by Bayesian optimization\n",
    "        if TEST_NULL_HYPO:\n",
    "            clf = LGBMClassifier(\n",
    "                nthread=int(multiprocessing.cpu_count()*CPU_USE_RATE),\n",
    "                n_estimators=10000,\n",
    "                learning_rate=0.02,\n",
    "                num_leaves=127,\n",
    "                max_depth=8,\n",
    "                silent=-1,\n",
    "                verbose=-1,\n",
    "                random_state=int(time.time()),\n",
    "                )\n",
    "        else:\n",
    "            clf = LGBMClassifier(\n",
    "                nthread=int(multiprocessing.cpu_count()*CPU_USE_RATE),\n",
    "                n_estimators=10000,\n",
    "                learning_rate=0.02,\n",
    "                num_leaves=34, # 20\n",
    "                colsample_bytree=0.2, #0.9497036 < 0.2\n",
    "                subsample=0.8715623,\n",
    "                max_depth=8, # 7\n",
    "                reg_alpha=0.041545473, # 0.3\n",
    "                reg_lambda=0.0735294,\n",
    "                min_split_gain=0.0222415,\n",
    "                min_child_weight=39.3259775, # 60\n",
    "                silent=-1,\n",
    "                verbose=-1,\n",
    "                random_state=int(time.time()),\n",
    "                )\n",
    "\n",
    "        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "            eval_metric= 'auc', verbose= False, early_stopping_rounds= 100) # early_stopping_rounds= 200\n",
    "        # training/validating\n",
    "        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "        train_preds[train_idx] += clf.predict_proba(train_x, num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n",
    "        # testing\n",
    "        sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "\n",
    "        logging.info('Fold %2d val AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n",
    "\n",
    "        del clf, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "    logging.info('Over-folds train AUC score : {}'.format(roc_auc_score(train_df['TARGET'], train_preds)))\n",
    "    \n",
    "    over_folds_val_auc = roc_auc_score(train_df['TARGET'], oof_preds)\n",
    "    logging.info('Over-folds val AUC score : {}'.format(over_folds_val_auc))\n",
    "    \n",
    "    # # Write submission file and plot feature importance\n",
    "    # test_df.loc[:,'TARGET'] = sub_preds\n",
    "    # test_df[['SK_ID_CURR', 'TARGET']].to_csv(submission_file_name, index= False)\n",
    "\n",
    "    return feature_importance_df, over_folds_val_auc\n",
    "\n",
    "def main():\n",
    "    #--------------------\n",
    "    # load features\n",
    "    #--------------------\n",
    "    input_dir = \"../features/filled_by_knn/\"\n",
    "    for f_p in os.listdir(input_dir):\n",
    "        df = pd.read_hdf(os.path.join(input_dir, f_p))\n",
    "        logging.info('loading features: {}'.format(f_p))\n",
    "        #--------------------\n",
    "        # out-of-fold validating stratigy + LGB\n",
    "        #--------------------    \n",
    "        with timer(\"Run LightGBM with kfold\"):\n",
    "            feature_importance_df = pd.DataFrame()\n",
    "            over_folds_val_auc_list = np.zeros(ITERATION)\n",
    "            for i in range(ITERATION):\n",
    "                logging.info('Iteration %i' %i)\n",
    "                iter_feat_imp, over_folds_val_auc = kfold_lightgbm(df, num_folds= NUM_FOLDS, stratified= STRATIFIED)\n",
    "                feature_importance_df = pd.concat([feature_importance_df, iter_feat_imp], axis=0)\n",
    "                over_folds_val_auc_list[i] = over_folds_val_auc\n",
    "\n",
    "            logging.info('Over-iterations val AUC score : {}'.format(over_folds_val_auc_list.mean()))\n",
    "            logging.info('Standard deviation : {}'.format(over_folds_val_auc_list.std()))\n",
    "\n",
    "            # display_importances(feature_importance_df)\n",
    "            feature_importance_df_median = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").median().sort_values(by=\"importance\", ascending=False)\n",
    "            useless_features_df = feature_importance_df_median.loc[feature_importance_df_median['importance'] == 0]\n",
    "            feature_importance_df_mean = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)\n",
    "            #---------------------\n",
    "            # save\n",
    "            #---------------------\n",
    "            output_path = '../output'\n",
    "            if not os.path.isdir(output_path):\n",
    "                os.mkdir(output_path)\n",
    "\n",
    "            if TEST_NULL_HYPO:\n",
    "                feature_importance_df_mean.to_csv(os.path.join(output_path, 'feature_importance-null_hypo.csv'), index = True)\n",
    "            else:\n",
    "                feature_importance_df_mean.to_csv(os.path.join(output_path, 'feature_importance_{}.csv'.format(f_p[20:-3])), index = True)\n",
    "                useless_features_list = useless_features_df.index.tolist()\n",
    "                logging.info('useless/overfitting features: \\'' + '\\', \\''.join(useless_features_list) + '\\'')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading features: base_featurs_filled_knn_10_w_target.h5\n",
      "Iteration 0\n",
      "no bugging in split\n",
      "Starting LightGBM. Train shape: (307507, 280), test shape: (48744, 280)\n",
      "Fold  1 val AUC : 0.798314\n",
      "Fold  2 val AUC : 0.792327\n",
      "Fold  3 val AUC : 0.789873\n",
      "Fold  4 val AUC : 0.791330\n",
      "Fold  5 val AUC : 0.792705\n",
      "Over-folds train AUC score : 0.8920738904511201\n",
      "Over-folds val AUC score : 0.7928974968032608\n",
      "Iteration 1\n",
      "no bugging in split\n",
      "Starting LightGBM. Train shape: (307507, 280), test shape: (48744, 280)\n",
      "Fold  1 val AUC : 0.794023\n"
     ]
    }
   ],
   "source": [
    "log_dir = '../log_imputating_exp'\n",
    "init_logging(log_dir)\n",
    "with timer(\"Lightgbm run a score\"):\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
