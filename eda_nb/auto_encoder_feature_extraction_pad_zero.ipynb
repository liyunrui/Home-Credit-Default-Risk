{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import sys\n",
    "sys.path.append('../py_model')\n",
    "from utils import init_logging\n",
    "import logging \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# auto-encoder network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder(dimensions=[278, 1000, 100, 278]):\n",
    "    \"\"\"\n",
    "    Build a stacked deep autoencoder with tied weights, that is w = wT.\n",
    "\n",
    "    return a dict.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dimensions : list, optional\n",
    "        The number of neurons for each layer of the autoencoder.\n",
    "    Returns\n",
    "    -------\n",
    "    x : Tensor\n",
    "        Input placeholder to the network\n",
    "    z : Tensor\n",
    "        Inner-most latent representation\n",
    "    y : Tensor\n",
    "        Output reconstruction of the input\n",
    "    loss : Tensor\n",
    "        Overall cost to use for training\n",
    "    \"\"\"    \n",
    "    logging.info('activation : {}'.format('relu'))\n",
    "    # input to the network\n",
    "    x = tf.placeholder(tf.float32, [None, dimensions[0]], name='x')\n",
    "    current_input = x\n",
    "    logging.info('current_input : {}'.format(current_input.shape))\n",
    "    #---------------------------\n",
    "    # Build the encoder\n",
    "    #---------------------------\n",
    "    encoder = [] # for putting the weight of encoder, w1,w2,..\n",
    "    for layer_i, n_output in enumerate(dimensions[1:]):\n",
    "        logging.info('layer_i-encoder : {}'.format(layer_i))\n",
    "        logging.info('n_output-encoder: {}'.format(n_output))\n",
    "        n_input = int(current_input.get_shape()[1]) # [0]: batch_szie, [1]:input_dim\n",
    "        logging.info('n_input : {}'.format(n_input))\n",
    "        W = tf.Variable(\n",
    "            tf.random_uniform([n_input, n_output],\n",
    "                              minval = -1.0 / math.sqrt(n_input),\n",
    "                              maxval = 1.0 / math.sqrt(n_input)))\n",
    "        b = tf.Variable(tf.zeros([n_output]))\n",
    "        # saving layer of encoding for decoder\n",
    "        encoder.append(W)\n",
    "        output = tf.nn.relu(tf.matmul(current_input, W) + b)\n",
    "        # assign current_input\n",
    "        current_input = output\n",
    "    #---------------------------\n",
    "    # latent representation (output of encoder)\n",
    "    #---------------------------\n",
    "    z = current_input\n",
    "    encoder.reverse() # [...,w2,w1]\n",
    "    \n",
    "    #---------------------------\n",
    "    # Build the decoder using the same weights\n",
    "    #---------------------------\n",
    "    for layer_i, n_output in enumerate(dimensions[:-1][::-1]):\n",
    "        logging.info('layer_i-decoder : {}'.format(layer_i))\n",
    "        logging.info('n_output-decoder : {}'.format(n_output))\n",
    "        W = tf.transpose(encoder[layer_i])\n",
    "        b = tf.Variable(tf.zeros([n_output]))\n",
    "        output = tf.nn.relu(tf.matmul(current_input, W) + b)\n",
    "        # assign current_input\n",
    "        current_input = output\n",
    "\n",
    "    # now have the reconstruction through the network\n",
    "    y = current_input\n",
    "    # Define loss and, minimize the mean squared error\n",
    "    loss = tf.reduce_mean(tf.pow(x - y, 2)) \n",
    "    return {'x': x, 'z': z, 'y': y, 'loss': loss}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = '../log/auto_encoder'\n",
    "checkpoint_dir = '../log/auto_encoder/checkpoints'\n",
    "prediction_dir = '../log/auto_encoder/predictions'\n",
    "init_logging(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(356251, 281)\n",
      "(356251, 278)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pad_zero : True\n",
      "input of ae : (356251, 278)\n"
     ]
    }
   ],
   "source": [
    "pad_zero = True\n",
    "# reload again for filling\n",
    "df = pd.read_hdf('../features/base_featurs.h5','base_featurs')\n",
    "print (df.shape)\n",
    "copy_for_the_following_merge = df[['SK_ID_CURR','TARGET']].copy()\n",
    "no_need_to_comoress = ['index','TARGET', 'SK_ID_CURR']\n",
    "df.drop(no_need_to_comoress, axis = 1, inplace = True)\n",
    "# handling with infinity\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
    "print (df.shape)\n",
    "if pad_zero == True:\n",
    "    # preprocessing for feature scailing\n",
    "    df.replace(np.nan, 0, inplace = True)\n",
    "    logging.info('pad_zero : {}'.format('True'))\n",
    "else:\n",
    "    logging.info('pad_zero : {}'.format('False'))\n",
    "# preprocessing for feature scailing ignoring nan\n",
    "for f in df.columns.tolist():\n",
    "    mean = df[f].mean()\n",
    "    std = df[f].std()\n",
    "    df[f] = (df[f] - mean) / std\n",
    "logging.info('input of ae : {}'.format(df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# raw_feature_generator\n",
    "### raw_feature_generator which allows to quickly set up Python generators that can automatically turn image files on disk into batches of pre-processed tensors. This is what we will use here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length 356251\n",
      "idx (356251,) [     0      1      2 ... 356248 356249 356250]\n"
     ]
    }
   ],
   "source": [
    "length = len(df)\n",
    "print ('length',length)\n",
    "idx = np.arange(length) # 1-D array\n",
    "print ('idx', idx.shape, idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def raw_feature_generator(batch_size = 128, shuffle = True, num_epochs = 10000, allow_smaller_final_batch = False):\n",
    "#     epoch_num = 0\n",
    "#     while epoch_num < num_epochs:\n",
    "#         if shuffle:\n",
    "#             np.random.shuffle(idx)\n",
    "#         for i in range(0, length, batch_size):\n",
    "#             batch_idx = idx[i: i + batch_size]\n",
    "#             if not allow_smaller_final_batch and len(batch_idx) != batch_size:\n",
    "#                 break # terminate the loop\n",
    "#             yield df.values[batch_idx]\n",
    "#         epoch_num += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_feature_generator(batch_size = 128, shuffle = True, allow_smaller_final_batch = False):\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx)\n",
    "    for i in range(0, length, batch_size):\n",
    "        batch_idx = idx[i: i + batch_size]\n",
    "        if not allow_smaller_final_batch and len(batch_idx) != batch_size:\n",
    "            break # terminate the loop\n",
    "        yield df.values[batch_idx]\n",
    "        \n",
    "def save(step, averaged = False):\n",
    "    '''\n",
    "    save the model\n",
    "    '''\n",
    "    #--------\n",
    "    # create saver object\n",
    "    #--------\n",
    "    if averaged:\n",
    "        saver = tf.train.Saver(self.ema.variables_to_restore(), max_to_keep=1)\n",
    "        checkpoint_dir_averaged = checkpoint_dir + '_avg'\n",
    "    else:\n",
    "        saver = tf.train.Saver(max_to_keep=1)\n",
    "    \n",
    "    if not os.path.isdir(checkpoint_dir):\n",
    "        logging.info('creating checkpoint directory {}'.format(checkpoint_dir))\n",
    "        os.mkdir(checkpoint_dir)\n",
    "\n",
    "    model_path = os.path.join(checkpoint_dir, 'model')\n",
    "    logging.info('saving model to {}'.format(model_path))\n",
    "    saver.save(sess, model_path)\n",
    "    \n",
    "def restore(step = None, averaged = False):\n",
    "    '''\n",
    "    restore the model.\n",
    "    \n",
    "    paras:\n",
    "    -------------\n",
    "    step: \n",
    "    '''\n",
    "    if averaged:\n",
    "        saver = tf.train.Saver(self.ema.variables_to_restore(), max_to_keep=1)\n",
    "        checkpoint_dir_averaged = checkpoint_dir + '_avg'\n",
    "    else:\n",
    "        saver = tf.train.Saver(max_to_keep=1)\n",
    "    #-------\n",
    "    # there are two way to use restore function based on if step is given.\n",
    "    #--------\n",
    "    if not step:\n",
    "        model_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "        logging.info('restoring model parameters from {}'.format(model_path))\n",
    "        saver.restore(sess, model_path)\n",
    "    else:\n",
    "        model_path = os.path.join(\n",
    "            checkpoint_dir, 'model{}-{}'.format('_avg' if averaged else '', step)\n",
    "        )\n",
    "        logging.info('restoring model from {}'.format(model_path))\n",
    "        saver.restore(sess, model_path)\n",
    "        \n",
    "def test_batch_generator(batch_size):\n",
    "    return self.batch_generator(\n",
    "        batch_size=batch_size,\n",
    "        df= self.test_df,\n",
    "        shuffle=False,\n",
    "        num_epochs=1,\n",
    "        is_test=True\n",
    "    )\n",
    "\n",
    "\n",
    "def batch_generator(batch_size, df, shuffle=True, num_epochs=10000, is_test=False):\n",
    "    batch_gen = df.batch_generator(batch_size, shuffle=shuffle, num_epochs=num_epochs, allow_smaller_final_batch=is_test)\n",
    "    for batch in batch_gen:\n",
    "        batch['order_dow_history'] = np.roll(batch['order_dow_history'], -1, axis=1)\n",
    "        batch['order_hour_history'] = np.roll(batch['order_hour_history'], -1, axis=1)\n",
    "        batch['days_since_prior_order_history'] = np.roll(batch['days_since_prior_order_history'], -1, axis=1)\n",
    "        batch['order_number_history'] = np.roll(batch['order_number_history'], -1, axis=1)\n",
    "        batch['next_is_ordered'] = np.roll(batch['is_ordered_history'], -1, axis=1)\n",
    "        batch['is_none'] = batch['product_id'] == 0\n",
    "        if not is_test:\n",
    "            batch['history_length'] = batch['history_length'] - 1\n",
    "        yield batch     \n",
    "        \n",
    "def predict(chunk_size = 2048, prediction_dir):\n",
    "    \n",
    "    if not os.path.isdir(prediction_dir):\n",
    "        os.makedirs(prediction_dir)\n",
    "\n",
    "\n",
    "    test_generator = test_batch_generator(chunk_size)\n",
    "    for i, test_batch_df in enumerate(test_generator):\n",
    "        if i % 100 == 0:\n",
    "            print i*chunk_size\n",
    "\n",
    "        test_feed_dict = {\n",
    "            getattr(self, placeholder_name, None): data\n",
    "            for placeholder_name, data in test_batch_df if hasattr(self, placeholder_name)\n",
    "        }\n",
    "        if hasattr(self, 'keep_prob'):\n",
    "            test_feed_dict.update({self.keep_prob: 1.0})\n",
    "        if hasattr(self, 'is_training'):\n",
    "            test_feed_dict.update({self.is_training: False})\n",
    "\n",
    "        tensor_names, tf_tensors = zip(*self.prediction_tensors.items())\n",
    "        np_tensors = self.session.run(\n",
    "            fetches=tf_tensors,\n",
    "            feed_dict=test_feed_dict\n",
    "        )\n",
    "        for tensor_name, tensor in zip(tensor_names, np_tensors):\n",
    "            prediction_dict[tensor_name].append(tensor)\n",
    "\n",
    "    for tensor_name, tensor in prediction_dict.items():\n",
    "        np_tensor = np.concatenate(tensor, 0)\n",
    "        save_file = os.path.join(self.prediction_dir, '{}.npy'.format(tensor_name))\n",
    "        logging.info('saving {} with shape {} to {}'.format(tensor_name, np_tensor.shape, save_file))\n",
    "        # save\n",
    "        np.save(save_file, np_tensor)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "activation : relu\n",
      "current_input : (?, 278)\n",
      "layer_i-encoder : 0\n",
      "n_output-encoder: 140\n",
      "n_input : 278\n",
      "layer_i-encoder : 1\n",
      "n_output-encoder: 70\n",
      "n_input : 140\n",
      "layer_i-encoder : 2\n",
      "n_output-encoder: 30\n",
      "n_input : 70\n",
      "layer_i-decoder : 0\n",
      "n_output-decoder : 70\n",
      "layer_i-decoder : 1\n",
      "n_output-decoder : 140\n",
      "layer_i-decoder : 2\n",
      "n_output-decoder : 278\n",
      "learning_rate : 0.001\n",
      "batch_size : 256\n",
      "n_epochs : 10\n",
      "log_interval : 100\n",
      "min_steps_to_checkpoint : 100\n",
      "enable_parameter_averaging : False\n",
      "[[epoch        0]]     [[loss at this epoch]]     loss: 3.96424152  \n",
      "saving model to ../log/auto_encoder/checkpoints/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 1391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[epoch        1]]     [[loss at this epoch]]     loss: 3.11947044  \n",
      "saving model to ../log/auto_encoder/checkpoints/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 2782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[epoch        2]]     [[loss at this epoch]]     loss: 2.8966567   \n",
      "saving model to ../log/auto_encoder/checkpoints/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 4173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[epoch        3]]     [[loss at this epoch]]     loss: 2.7810074   \n",
      "saving model to ../log/auto_encoder/checkpoints/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 5564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[epoch        4]]     [[loss at this epoch]]     loss: 2.68471283  \n",
      "saving model to ../log/auto_encoder/checkpoints/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 6955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[epoch        5]]     [[loss at this epoch]]     loss: 2.60682707  \n",
      "saving model to ../log/auto_encoder/checkpoints/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 8346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[epoch        6]]     [[loss at this epoch]]     loss: 2.55648368  \n",
      "saving model to ../log/auto_encoder/checkpoints/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 9737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[epoch        7]]     [[loss at this epoch]]     loss: 2.50566654  \n",
      "saving model to ../log/auto_encoder/checkpoints/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 11128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[epoch        8]]     [[loss at this epoch]]     loss: 2.46484468  \n",
      "saving model to ../log/auto_encoder/checkpoints/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 12519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[epoch        9]]     [[loss at this epoch]]     loss: 2.43207278  \n",
      "saving model to ../log/auto_encoder/checkpoints/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 13910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "restoring model parameters from ../log/auto_encoder/checkpoints/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../log/auto_encoder/checkpoints/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ../log/auto_encoder/checkpoints/model\n"
     ]
    }
   ],
   "source": [
    "# display_step\n",
    "log_interval = 100\n",
    "# learning_rate\n",
    "learning_rate = 0.001\n",
    "# define auto-encoder network architecture\n",
    "ae = autoencoder(dimensions=[278, 140, 70, 30])\n",
    "# optimizer\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(ae['loss'])\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "# We create a session to use the graph\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Training\n",
    "batch_size = 256\n",
    "n_epochs = 10\n",
    "step = 0\n",
    "min_steps_to_checkpoint = 100\n",
    "enable_parameter_averaging = False\n",
    "logging.info('learning_rate : {}'.format(learning_rate))\n",
    "logging.info('batch_size : {}'.format(batch_size))\n",
    "logging.info('n_epochs : {}'.format(n_epochs))\n",
    "logging.info('log_interval : {}'.format(log_interval))\n",
    "logging.info('min_steps_to_checkpoint : {}'.format(min_steps_to_checkpoint))\n",
    "logging.info('enable_parameter_averaging : {}'.format(enable_parameter_averaging))\n",
    "\n",
    "#-------------------------\n",
    "# training - fit\n",
    "#-------------------------\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_cost = 0.0\n",
    "    for batch_xs in raw_feature_generator(batch_size = batch_size):\n",
    "        _, batch_loss = sess.run([optimizer, ae['loss']], feed_dict={ae['x']: batch_xs})\n",
    "        epoch_cost += batch_loss / batch_size\n",
    "        step += 1\n",
    "    metric_log = (\n",
    "                \"[[epoch {:>8}]]     \"\n",
    "                \"[[loss at this epoch]]     loss: {:<12}\"\n",
    "            ).format(epoch, round(epoch_cost, 8))\n",
    "    logging.info(metric_log)\n",
    "    if step > min_steps_to_checkpoint:\n",
    "        print ('step : {}'.format(step))\n",
    "        save(step)\n",
    "        if enable_parameter_averaging:\n",
    "            save(step, averaged=True)\n",
    "restore()\n",
    "predict(prediction_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
