{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# auto-encoder network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder(dimensions=[278, 140, 70, 30]):\n",
    "    \"\"\"\n",
    "    Build a stacked deep autoencoder with tied weights, that is w = wT.\n",
    "\n",
    "    return a dict.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dimensions : list, optional\n",
    "        The number of neurons for each layer of the autoencoder.\n",
    "    Returns\n",
    "    -------\n",
    "    x : Tensor\n",
    "        Input placeholder to the network\n",
    "    z : Tensor\n",
    "        Inner-most latent representation\n",
    "    y : Tensor\n",
    "        Output reconstruction of the input\n",
    "    loss : Tensor\n",
    "        Overall cost to use for training\n",
    "    \"\"\"    \n",
    "    # input to the network\n",
    "    x = tf.placeholder(tf.float32, [None, dimensions[0]], name='x')\n",
    "    current_input = x\n",
    "    print ('current_input', current_input.shape)\n",
    "    #---------------------------\n",
    "    # Build the encoder\n",
    "    #---------------------------\n",
    "    encoder = [] # for putting the weight of encoder, w1,w2,..\n",
    "    for layer_i, n_output in enumerate(dimensions[1:]):\n",
    "        print ('layer_i-encoder', layer_i)\n",
    "        print ('n_output-encoder', n_output)\n",
    "        n_input = int(current_input.get_shape()[1]) # [0]: batch_szie, [1]:input_dim\n",
    "        print ('n_input', n_input)\n",
    "        W = tf.Variable(\n",
    "            tf.random_uniform([n_input, n_output],\n",
    "                              minval = -1.0 / math.sqrt(n_input),\n",
    "                              maxval = 1.0 / math.sqrt(n_input)))\n",
    "        b = tf.Variable(tf.zeros([n_output]))\n",
    "        # saving layer of encoding for decoder\n",
    "        encoder.append(W)\n",
    "        output = tf.nn.tanh(tf.matmul(current_input, W) + b)\n",
    "        # assign current_input\n",
    "        current_input = output\n",
    "    #---------------------------\n",
    "    # latent representation (output of encoder)\n",
    "    #---------------------------\n",
    "    z = current_input\n",
    "    encoder.reverse() # [...,w2,w1]\n",
    "    \n",
    "    #---------------------------\n",
    "    # Build the decoder using the same weights\n",
    "    #---------------------------\n",
    "    for layer_i, n_output in enumerate(dimensions[:-1][::-1]):\n",
    "        print ('layer_i-decoder', layer_i)\n",
    "        print ('n_output-decoder', n_output)\n",
    "        W = tf.transpose(encoder[layer_i])\n",
    "        b = tf.Variable(tf.zeros([n_output]))\n",
    "        output = tf.nn.tanh(tf.matmul(current_input, W) + b)\n",
    "        # assign current_input\n",
    "        current_input = output\n",
    "\n",
    "    # now have the reconstruction through the network\n",
    "    y = current_input\n",
    "    # Define loss and, minimize the mean squared error\n",
    "    loss = tf.reduce_mean(tf.pow(x - y, 2)) \n",
    "\n",
    "    return {'x': x, 'z': z, 'y': y, 'loss': loss}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(356251, 281)\n",
      "(356251, 278)\n"
     ]
    }
   ],
   "source": [
    "# reload again for filling\n",
    "df = pd.read_hdf('../features/base_featurs.h5','base_featurs')\n",
    "print (df.shape)\n",
    "copy_for_the_following_merge = df[['SK_ID_CURR','TARGET']].copy()\n",
    "no_need_to_comoress = ['index','TARGET', 'SK_ID_CURR']\n",
    "df.drop(no_need_to_comoress, axis = 1, inplace = True)\n",
    "# handling with infinity\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
    "print (df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define auto-encoder network architecture\n",
    "ae = autoencoder(dimensions=[278, 140, 70, 30])\n",
    "# optimizer\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(ae['loss'])\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "# We create a session to use the graph\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Fit all training data\n",
    "batch_size = 128\n",
    "n_epochs = 10\n",
    "for epoch_i in range(n_epochs):\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
