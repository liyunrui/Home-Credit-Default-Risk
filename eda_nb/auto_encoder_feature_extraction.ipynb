{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# auto-encoder network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder(dimensions=[278, 140, 70, 30]):\n",
    "    \"\"\"\n",
    "    Build a stacked deep autoencoder with tied weights, that is w = wT.\n",
    "\n",
    "    return a dict.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dimensions : list, optional\n",
    "        The number of neurons for each layer of the autoencoder.\n",
    "    Returns\n",
    "    -------\n",
    "    x : Tensor\n",
    "        Input placeholder to the network\n",
    "    z : Tensor\n",
    "        Inner-most latent representation\n",
    "    y : Tensor\n",
    "        Output reconstruction of the input\n",
    "    loss : Tensor\n",
    "        Overall cost to use for training\n",
    "    \"\"\"    \n",
    "    # input to the network\n",
    "    x = tf.placeholder(tf.float32, [None, dimensions[0]], name='x')\n",
    "    current_input = x\n",
    "    print ('current_input', current_input.shape)\n",
    "    #---------------------------\n",
    "    # Build the encoder\n",
    "    #---------------------------\n",
    "    encoder = [] # for putting the weight of encoder, w1,w2,..\n",
    "    for layer_i, n_output in enumerate(dimensions[1:]):\n",
    "        print ('layer_i-encoder', layer_i)\n",
    "        print ('n_output-encoder', n_output)\n",
    "        n_input = int(current_input.get_shape()[1]) # [0]: batch_szie, [1]:input_dim\n",
    "        print ('n_input', n_input)\n",
    "        W = tf.Variable(\n",
    "            tf.random_uniform([n_input, n_output],\n",
    "                              minval = -1.0 / math.sqrt(n_input),\n",
    "                              maxval = 1.0 / math.sqrt(n_input)))\n",
    "        b = tf.Variable(tf.zeros([n_output]))\n",
    "        # saving layer of encoding for decoder\n",
    "        encoder.append(W)\n",
    "        output = tf.nn.tanh(tf.matmul(current_input, W) + b)\n",
    "        # assign current_input\n",
    "        current_input = output\n",
    "    #---------------------------\n",
    "    # latent representation (output of encoder)\n",
    "    #---------------------------\n",
    "    z = current_input\n",
    "    encoder.reverse() # [...,w2,w1]\n",
    "    \n",
    "    #---------------------------\n",
    "    # Build the decoder using the same weights\n",
    "    #---------------------------\n",
    "    for layer_i, n_output in enumerate(dimensions[:-1][::-1]):\n",
    "        print ('layer_i-decoder', layer_i)\n",
    "        print ('n_output-decoder', n_output)\n",
    "        W = tf.transpose(encoder[layer_i])\n",
    "        b = tf.Variable(tf.zeros([n_output]))\n",
    "        output = tf.nn.tanh(tf.matmul(current_input, W) + b)\n",
    "        # assign current_input\n",
    "        current_input = output\n",
    "\n",
    "    # now have the reconstruction through the network\n",
    "    y = current_input\n",
    "    # Define loss and, minimize the mean squared error\n",
    "    loss = tf.reduce_mean(tf.pow(x - y, 2)) \n",
    "\n",
    "    return {'x': x, 'z': z, 'y': y, 'loss': loss}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(356251, 281)\n",
      "(356251, 278)\n"
     ]
    }
   ],
   "source": [
    "# reload again for filling\n",
    "df = pd.read_hdf('../features/base_featurs.h5','base_featurs')\n",
    "print (df.shape)\n",
    "copy_for_the_following_merge = df[['SK_ID_CURR','TARGET']].copy()\n",
    "no_need_to_comoress = ['index','TARGET', 'SK_ID_CURR']\n",
    "df.drop(no_need_to_comoress, axis = 1, inplace = True)\n",
    "# handling with infinity\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
    "print (df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# raw_feature_generator\n",
    "### raw_feature_generator which allows to quickly set up Python generators that can automatically turn image files on disk into batches of pre-processed tensors. This is what we will use here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length 356251\n",
      "idx (356251,) [     0      1      2 ... 356248 356249 356250]\n"
     ]
    }
   ],
   "source": [
    "length = len(df)\n",
    "print ('length',length)\n",
    "idx = np.arange(length) # 1-D array\n",
    "print ('idx', idx.shape, idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_feature_generator(batch_size = 128, shuffle = True, num_epochs = 10000, allow_smaller_final_batch = False):\n",
    "    epoch_num = 0\n",
    "    while epoch_num < num_epochs:\n",
    "        print ('epoch_num : {}'.format(epoch_num))\n",
    "        if shuffle:\n",
    "            np.random.shuffle(idx)\n",
    "        for i in range(0, length, batch_size):\n",
    "            batch_idx = idx[i: i + batch_size]\n",
    "            if not allow_smaller_final_batch and len(batch_idx) != batch_size:\n",
    "                break # terminate the loop\n",
    "            yield df.values[batch_idx]\n",
    "        epoch_num += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_input (?, 278)\n",
      "layer_i-encoder 0\n",
      "n_output-encoder 140\n",
      "n_input 278\n",
      "layer_i-encoder 1\n",
      "n_output-encoder 70\n",
      "n_input 140\n",
      "layer_i-encoder 2\n",
      "n_output-encoder 30\n",
      "n_input 70\n",
      "layer_i-decoder 0\n",
      "n_output-decoder 70\n",
      "layer_i-decoder 1\n",
      "n_output-decoder 140\n",
      "layer_i-decoder 2\n",
      "n_output-decoder 278\n",
      "epoch_num : 0\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n",
      "batch_xs (128, 278)\n",
      "Step 0: Minibatch Loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-2f281bb591b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_xs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_feature_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'batch_xs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m#-------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-debc3dec2fb1>\u001b[0m in \u001b[0;36mraw_feature_generator\u001b[0;34m(batch_size, shuffle, num_epochs, allow_smaller_final_batch)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_smaller_final_batch\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0;32mbreak\u001b[0m \u001b[0;31m# terminate the loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mepoch_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mvalues\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4631\u001b[0m         \"\"\"\n\u001b[1;32m   4632\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4633\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_AXIS_REVERSED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4635\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mas_array\u001b[0;34m(self, transpose, items)\u001b[0m\n\u001b[1;32m   3947\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3948\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3949\u001b[0;31m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3951\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtranspose\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m_interleave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3976\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3977\u001b[0m             \u001b[0mrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3978\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3979\u001b[0m             \u001b[0mitemmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# display_step\n",
    "display_step = 1000\n",
    "# learning_rate\n",
    "learning_rate = 0.001\n",
    "# define auto-encoder network architecture\n",
    "ae = autoencoder(dimensions=[278, 140, 70, 30])\n",
    "# optimizer\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(ae['loss'])\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "# We create a session to use the graph\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Training\n",
    "batch_size = 128\n",
    "n_epochs = 10000\n",
    "for batch_xs in raw_feature_generator(batch_size = 128, num_epochs = 10000):\n",
    "    print ('batch_xs', batch_xs.shape)\n",
    "    #-------------------------\n",
    "    # feature scaling: make different feature have the same scaling\n",
    "    #-------------------------\n",
    "    #train = np.array([img - mean_img for img in batch_xs])\n",
    "    # train\n",
    "    _, loss = sess.run([optimizer, ae['loss']], feed_dict={ae['x']: batch_xs})\n",
    "    if i % display_step == 0 or i == 1:\n",
    "        print('Step %i: Minibatch Loss: %f' % (i, loss))\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
